{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Basic Custom Training Container</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to build and use a basic custom Docker container for training with Amazon SageMaker. Reference documentation is available at https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining some variables like the current execution role, the ECR repository that we are going to use for pushing the custom Docker container and a default Amazon S3 bucket to be used by Amazon SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "ecr_namespace = 'aws_batch_tesseract/'\n",
    "prefix = 'basic'\n",
    "\n",
    "ecr_repository_name = ecr_namespace + prefix\n",
    "role = get_execution_role()\n",
    "account_id = role.split(':')[4]\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(account_id)\n",
    "print(region)\n",
    "print(role)\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the Dockerfile which defines the statements for building our custom SageMaker training container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize ./TESSERACT-SAGEMAKER-CONTAINER/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At high-level the Dockerfile specifies the following operations for building this container:\n",
    "<ul>\n",
    "    <li>Start from Ubuntu 18.04</li>\n",
    "    <li>Define some variables to be used at build time to install Python 3</li>\n",
    "    <li>Some handful libraries are installed with apt-get</li>\n",
    "    <li>tesseract and pdfsandwitch related libraries including fonts used for training</li>\n",
    "    <li>We then install Python 3 and create a symbolic link</li>\n",
    "    <li>We install some Python libraries like numpy, pandas, ScikitLearn, etc.</li>\n",
    "    <li>We set e few environment variables, including PYTHONUNBUFFERED which is used to avoid buffering Python standard output (useful for logging)</li>\n",
    "    <li>Finally, we copy all contents in <strong>code/</strong> (which is where our training code is) to the WORKDIR </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build and push the container</h3>\n",
    "We are now ready to build this container and push it to Amazon ECR. This task is executed using a shell script stored in the ../script/ folder. Let's take a look at this script and then execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize ./TESSERACT-SAGEMAKER-CONTAINER/container_build_script/build_and_push.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>--------------------------------------------------------------------------------------------------------------------</h3>\n",
    "\n",
    "The script builds the Docker container, then creates the repository if it does not exist, and finally pushes the container to the ECR repository. The build task requires a few minutes to be executed the first time, then Docker caches build outputs to be reused for the subsequent build operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!  bash ./TESSERACT-SAGEMAKER-CONTAINER/container_build_script/build_and_push.sh $account_id $region $ecr_repository_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training with Amazon SageMaker</h3>\n",
    "\n",
    "Once we have correctly pushed our container to Amazon ECR, we are ready to start training with Amazon SageMaker, which requires the ECR path to the Docker container used for training as parameter for starting a training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_image_uri = '{0}.dkr.ecr.{1}.amazonaws.com/{2}:latest'.format(account_id, region, ecr_repository_name)\n",
    "print(container_image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main scripts invoked by Sagemaker:\n",
    "    - train: invoked when running a training process\n",
    "    - serve: invoked when serving and endpoint or to start a Batch serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For integration with custom algorithms Sagemaker uses the following structure:\n",
    "/opt/ml  \n",
    "├── input  \n",
    "│   ├── config  \n",
    "│   │   ├── hyperparameters.json            <--- Hyper Parameters passed to the script when invoking sagemaker  \n",
    "│   │   └── resourceConfig.json             <--- Configuration to access input / test / validation data  \n",
    "│   └── data  \n",
    "│       └── channel_name/                   <--- Where data are downloaded by Sagemaker  \n",
    "│                            \n",
    "├── model                                   <--- Output directory where model shall be stored when training  \n",
    "│                                                Also used to store model when starting a prediction  \n",
    "├── code                                    <--- Custom script files  \n",
    "│  \n",
    "└── output                                  <--- Output folder for predictions\n",
    "└── failure                                 <--- Store here error descriptions that will be reported to the user\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./TESSERACT-SAGEMAKER-CONTAINER/code/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can execute the training job by calling the fit() method of the generic Estimator object defined in the Amazon SageMaker Python SDK (https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/estimator.py). This corresponds to calling the CreateTrainingJob() API (https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "#Configure training Job parameters and select type of server to perform the training\n",
    "#Tesseract is not able to leverage machines bigger than ml.c5.2xlarge for training\n",
    "\n",
    "est = sagemaker.estimator.Estimator(container_image_uri,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    #train_instance_type='local', # use local mode\n",
    "                                    train_instance_type='ml.c5.2xlarge',\n",
    "                                    base_job_name=prefix+\"EMPTYTrainedData\"\n",
    "                                    )\n",
    "\n",
    "est.set_hyperparameters(epoch=1)\n",
    "\n",
    "train_data = sagemaker.session.s3_input('s3://348831852500-sagemaker-us-east-1/Tesseract/empty_training/')\n",
    "\n",
    "est.fit({'train': train_data })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(est._current_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_basic_model='basicEMPTYTrainedData-2020-01-28-09-33-44-466'\n",
    "trained_model='basicFullTrainedData-2020-01-27-12-25-45-080'\n",
    "#Full Training: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perdiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction process works in the same way for Batch Prediction Jobs and Online Inferences as well.\n",
    "It uses API calls to pass informations to the Sagemaker Container using the following endpoints:\n",
    "- ping                   <--- to check if the container is health\n",
    "- invocations            <--- to send data for preditcion and get back the answer\n",
    "- execution_parameters   <--- Used only for batch prediction to define how the batch process works\n",
    "\n",
    "The container is started invoking the serve scripts in the standard approach this script do not need editing and starts: \n",
    "- an nginx server to expose an http endpoint for the requests\n",
    "- an gunicorn server to receive and process requests using Flask (python) that calls predictor.py\n",
    "\n",
    "The customization is mainly in the predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./TESSERACT-SAGEMAKER-CONTAINER/code/predictor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data for a bacth request and copy them to an input s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat ./sample-data/process.csv\n",
    "#InputBucket,#InputPath,#InputFileName,#OutputBucket,#OutputPath,#OutputFileName,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp ./sample-data/process.csv s3://348831852500-sagemaker-us-east-1/Tesseract/process/process.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = 's3://348831852500-sagemaker-us-east-1/Tesseract/process/process.csv'\n",
    "\n",
    "# The location to store the results of the batch transform job\n",
    "batch_output = 's3://348831852500-sagemaker-us-east-1/Tesseract/Output/'\n",
    "\n",
    "\n",
    "batch=est.transformer (\n",
    "                       instance_count=1, \n",
    "                       instance_type='ml.c5.4xlarge', \n",
    "                       #instance_type='local', # use local mode\n",
    "                       output_path=batch_output,\n",
    "                       strategy =\"SingleRecord\",\n",
    "                       max_concurrent_transforms=1\n",
    ")\n",
    "\n",
    "#batch=sagemaker.transformer.Transformer (\n",
    "#                       model_name=empty_basic_model,\n",
    "#                       instance_count=1, \n",
    "#                       instance_type='ml.c5.4xlarge', \n",
    "#                       #instance_type='local', # use local mode\n",
    "#                       output_path=batch_output,\n",
    "#                       strategy =\"SingleRecord\",\n",
    "#                       max_concurrent_transforms=1\n",
    "#)\n",
    "\n",
    "batch.transform(data=batch_input, data_type='S3Prefix',  content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker supports also on-line prediction \n",
    "For this specific process may not be the best cost effective solution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint=est.deploy (initial_instance_count=1, \n",
    "                     instance_type='ml.c5.9xlarge' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input_csv=[\"348831852500-sagemaker-us-east-1,Tesseract/Input,Leg_001_DA00_cat_002.pdf,348831852500-sagemaker-us-east-1,Tesseract/Output,OUT_Leg_001_DA00_cat_002.pdf\"]\n",
    "\n",
    "ret=endpoint.predict (json_string)\n",
    "\n",
    "print (ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = 's3://348831852500-sagemaker-us-east-1/Tesseract/process/process.csv'\n",
    "\n",
    "# The location to store the results of the batch transform job\n",
    "batch_output = 's3://348831852500-sagemaker-us-east-1/Tesseract/Output/'\n",
    "tra.transform(data=batch_input, data_type='S3Prefix',  content_type='text/csv', split_type='Line')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
